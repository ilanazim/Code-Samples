{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### Assignment 2: Spring Quarter Data Science, Low-cost Sensor - Decision Trees\nIlana Zimmerman 4/12/18"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Assignment Description\nYour task for this assignment:  Design a simple, low-cost sensor that can distinguish between red wine and white wine.\n\nYour sensor must correctly distinguish between red and white wine for at least 95% of the samples in a set of 6497 test samples of red and white wine.\nYour technology is capable of sensing the following wine attributes:\n\n    Fixed acidity  -  Free sulphur dioxide\n    Volatile acidity  -  Total sulphur dioxide\n    Citric acid  -  Sulphates\n    Residual sugar  -  pH\n    Chlorides  - Alcohol\n    Density\n\n** Decision Trees explained: http://www.saedsayad.com/decision_tree.htm"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### In this assignment we were asked to build an experiment using Decision Trees and answer:\n\n   1. What is the percentage of correct classification results (using all attributes)?\n   2. What is the percentage of correct classification results (using a subset of the attributes)?\n   3. What is the AUC of your model?\n   4. Visualize your decision tree\n   5. What is the best AUC that you can achieve?\n   6. Which are the the minimum number of attributes? Why?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### What Data Are We Working With?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\ndf_wine = pd.read_csv('RedWhiteWine.csv', delimiter = ',')\n\nprint(df_wine.shape)\n\n#print first 5 rows of the dataset\ndf_wine.head(5)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(6497, 13)\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.88</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.9968</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.76</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.9970</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.28</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.9980</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.4              0.70         0.00             1.9      0.076   \n1            7.8              0.88         0.00             2.6      0.098   \n2            7.8              0.76         0.04             2.3      0.092   \n3           11.2              0.28         0.56             1.9      0.075   \n4            7.4              0.70         0.00             1.9      0.076   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 11.0                  34.0   0.9978  3.51       0.56   \n1                 25.0                  67.0   0.9968  3.20       0.68   \n2                 15.0                  54.0   0.9970  3.26       0.65   \n3                 17.0                  60.0   0.9980  3.16       0.58   \n4                 11.0                  34.0   0.9978  3.51       0.56   \n\n   alcohol  quality  Class  \n0      9.4        5      1  \n1      9.8        5      1  \n2      9.8        5      1  \n3      9.8        6      1  \n4      9.4        5      1  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_wine=df_wine.drop('quality', axis = 1)",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Split the Data into Test/Train Sets"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = df_wine.iloc[:,0:11] #X includes all rows and columns EXCEPT the 'Class' column which we are predicting\ny = df_wine['Class']\n#Split data into test and train data, 20:80\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.2)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)  ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(5197, 11) (1300, 11) (5197,) (1300,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Generate the Model : Decision Tree Classifier"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tree = DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 2, random_state=5)#HOW TO CHOOSE CRITERION/RANDOM STATE\ntree.fit(X_train, y_train)\ny_predict = tree.predict(X_test)\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_predict",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "array([0, 0, 1, ..., 0, 1, 0])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Evaluate the Model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Question 1: Model Accuracy Using ALL Features"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def accuracy_score(y_test, y_predict):\n    test_acc = np.mean([a==p for a, p in zip(y_test, y_predict)])\n    return test_acc",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Determine accuracy of these predictions\nfrom sklearn import metrics\naccuracy_score(y_test, y_predict)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "0.9830769230769231"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (metrics.confusion_matrix( y_test, y_predict, labels=None))",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[976  14]\n [  8 302]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Question 2: Model Accuracy Using a Subset of Features"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import random\nfrom itertools import combinations",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def Feature_Selection(Columns, i):\n    #iterate through X matrix in any dimension [6497, a], not including 'Class' column\n   # i=random.choice(range(1,11))\n    for r in range(i+1):\n        features = list(a for a in combinations (Columns, r) if a !=[] )#list of all possible combinations given 'a', not am empty array!\n        X_new = np.asarray((features))\n    return X_new;\n    ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#define columns from dataframe - list of all headers; drop target column\ncolumns = list(df_wine)\ncolumns.remove('Class')\ncolumns = np.asarray(columns)\n",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Test that Feature Selection chooses a different number of featuers randomly\ndf_wine[Feature_Selection(columns, 3)[2]]",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>chlorides</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.076</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.880</td>\n      <td>0.098</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.760</td>\n      <td>0.092</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.280</td>\n      <td>0.075</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.076</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7.4</td>\n      <td>0.660</td>\n      <td>0.075</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7.9</td>\n      <td>0.600</td>\n      <td>0.069</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7.3</td>\n      <td>0.650</td>\n      <td>0.065</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7.8</td>\n      <td>0.580</td>\n      <td>0.073</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7.5</td>\n      <td>0.500</td>\n      <td>0.071</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>6.7</td>\n      <td>0.580</td>\n      <td>0.097</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7.5</td>\n      <td>0.500</td>\n      <td>0.071</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5.6</td>\n      <td>0.615</td>\n      <td>0.089</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7.8</td>\n      <td>0.610</td>\n      <td>0.114</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>8.9</td>\n      <td>0.620</td>\n      <td>0.176</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>8.9</td>\n      <td>0.620</td>\n      <td>0.170</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>8.5</td>\n      <td>0.280</td>\n      <td>0.092</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>8.1</td>\n      <td>0.560</td>\n      <td>0.368</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7.4</td>\n      <td>0.590</td>\n      <td>0.086</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>7.9</td>\n      <td>0.320</td>\n      <td>0.341</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>8.9</td>\n      <td>0.220</td>\n      <td>0.077</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>7.6</td>\n      <td>0.390</td>\n      <td>0.082</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>7.9</td>\n      <td>0.430</td>\n      <td>0.106</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>8.5</td>\n      <td>0.490</td>\n      <td>0.084</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>6.9</td>\n      <td>0.400</td>\n      <td>0.085</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>6.3</td>\n      <td>0.390</td>\n      <td>0.080</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7.6</td>\n      <td>0.410</td>\n      <td>0.080</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>7.9</td>\n      <td>0.430</td>\n      <td>0.106</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7.1</td>\n      <td>0.710</td>\n      <td>0.080</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7.8</td>\n      <td>0.645</td>\n      <td>0.082</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6467</th>\n      <td>5.8</td>\n      <td>0.230</td>\n      <td>0.046</td>\n    </tr>\n    <tr>\n      <th>6468</th>\n      <td>6.6</td>\n      <td>0.240</td>\n      <td>0.032</td>\n    </tr>\n    <tr>\n      <th>6469</th>\n      <td>6.1</td>\n      <td>0.320</td>\n      <td>0.021</td>\n    </tr>\n    <tr>\n      <th>6470</th>\n      <td>5.0</td>\n      <td>0.200</td>\n      <td>0.015</td>\n    </tr>\n    <tr>\n      <th>6471</th>\n      <td>6.0</td>\n      <td>0.420</td>\n      <td>0.032</td>\n    </tr>\n    <tr>\n      <th>6472</th>\n      <td>5.7</td>\n      <td>0.210</td>\n      <td>0.030</td>\n    </tr>\n    <tr>\n      <th>6473</th>\n      <td>5.6</td>\n      <td>0.200</td>\n      <td>0.048</td>\n    </tr>\n    <tr>\n      <th>6474</th>\n      <td>7.4</td>\n      <td>0.220</td>\n      <td>0.035</td>\n    </tr>\n    <tr>\n      <th>6475</th>\n      <td>6.2</td>\n      <td>0.380</td>\n      <td>0.038</td>\n    </tr>\n    <tr>\n      <th>6476</th>\n      <td>5.9</td>\n      <td>0.540</td>\n      <td>0.032</td>\n    </tr>\n    <tr>\n      <th>6477</th>\n      <td>6.2</td>\n      <td>0.530</td>\n      <td>0.035</td>\n    </tr>\n    <tr>\n      <th>6478</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.046</td>\n    </tr>\n    <tr>\n      <th>6479</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.046</td>\n    </tr>\n    <tr>\n      <th>6480</th>\n      <td>5.0</td>\n      <td>0.235</td>\n      <td>0.030</td>\n    </tr>\n    <tr>\n      <th>6481</th>\n      <td>5.5</td>\n      <td>0.320</td>\n      <td>0.037</td>\n    </tr>\n    <tr>\n      <th>6482</th>\n      <td>4.9</td>\n      <td>0.470</td>\n      <td>0.035</td>\n    </tr>\n    <tr>\n      <th>6483</th>\n      <td>6.5</td>\n      <td>0.330</td>\n      <td>0.048</td>\n    </tr>\n    <tr>\n      <th>6484</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.046</td>\n    </tr>\n    <tr>\n      <th>6485</th>\n      <td>6.2</td>\n      <td>0.210</td>\n      <td>0.028</td>\n    </tr>\n    <tr>\n      <th>6486</th>\n      <td>6.2</td>\n      <td>0.410</td>\n      <td>0.023</td>\n    </tr>\n    <tr>\n      <th>6487</th>\n      <td>6.8</td>\n      <td>0.220</td>\n      <td>0.052</td>\n    </tr>\n    <tr>\n      <th>6488</th>\n      <td>4.9</td>\n      <td>0.235</td>\n      <td>0.030</td>\n    </tr>\n    <tr>\n      <th>6489</th>\n      <td>6.1</td>\n      <td>0.340</td>\n      <td>0.036</td>\n    </tr>\n    <tr>\n      <th>6490</th>\n      <td>5.7</td>\n      <td>0.210</td>\n      <td>0.038</td>\n    </tr>\n    <tr>\n      <th>6491</th>\n      <td>6.5</td>\n      <td>0.230</td>\n      <td>0.032</td>\n    </tr>\n    <tr>\n      <th>6492</th>\n      <td>6.2</td>\n      <td>0.210</td>\n      <td>0.039</td>\n    </tr>\n    <tr>\n      <th>6493</th>\n      <td>6.6</td>\n      <td>0.320</td>\n      <td>0.047</td>\n    </tr>\n    <tr>\n      <th>6494</th>\n      <td>6.5</td>\n      <td>0.240</td>\n      <td>0.041</td>\n    </tr>\n    <tr>\n      <th>6495</th>\n      <td>5.5</td>\n      <td>0.290</td>\n      <td>0.022</td>\n    </tr>\n    <tr>\n      <th>6496</th>\n      <td>6.0</td>\n      <td>0.210</td>\n      <td>0.020</td>\n    </tr>\n  </tbody>\n</table>\n<p>6497 rows Ã— 3 columns</p>\n</div>",
            "text/plain": "      fixed acidity  volatile acidity  chlorides\n0               7.4             0.700      0.076\n1               7.8             0.880      0.098\n2               7.8             0.760      0.092\n3              11.2             0.280      0.075\n4               7.4             0.700      0.076\n5               7.4             0.660      0.075\n6               7.9             0.600      0.069\n7               7.3             0.650      0.065\n8               7.8             0.580      0.073\n9               7.5             0.500      0.071\n10              6.7             0.580      0.097\n11              7.5             0.500      0.071\n12              5.6             0.615      0.089\n13              7.8             0.610      0.114\n14              8.9             0.620      0.176\n15              8.9             0.620      0.170\n16              8.5             0.280      0.092\n17              8.1             0.560      0.368\n18              7.4             0.590      0.086\n19              7.9             0.320      0.341\n20              8.9             0.220      0.077\n21              7.6             0.390      0.082\n22              7.9             0.430      0.106\n23              8.5             0.490      0.084\n24              6.9             0.400      0.085\n25              6.3             0.390      0.080\n26              7.6             0.410      0.080\n27              7.9             0.430      0.106\n28              7.1             0.710      0.080\n29              7.8             0.645      0.082\n...             ...               ...        ...\n6467            5.8             0.230      0.046\n6468            6.6             0.240      0.032\n6469            6.1             0.320      0.021\n6470            5.0             0.200      0.015\n6471            6.0             0.420      0.032\n6472            5.7             0.210      0.030\n6473            5.6             0.200      0.048\n6474            7.4             0.220      0.035\n6475            6.2             0.380      0.038\n6476            5.9             0.540      0.032\n6477            6.2             0.530      0.035\n6478            6.6             0.340      0.046\n6479            6.6             0.340      0.046\n6480            5.0             0.235      0.030\n6481            5.5             0.320      0.037\n6482            4.9             0.470      0.035\n6483            6.5             0.330      0.048\n6484            6.6             0.340      0.046\n6485            6.2             0.210      0.028\n6486            6.2             0.410      0.023\n6487            6.8             0.220      0.052\n6488            4.9             0.235      0.030\n6489            6.1             0.340      0.036\n6490            5.7             0.210      0.038\n6491            6.5             0.230      0.032\n6492            6.2             0.210      0.039\n6493            6.6             0.320      0.047\n6494            6.5             0.240      0.041\n6495            5.5             0.290      0.022\n6496            6.0             0.210      0.020\n\n[6497 rows x 3 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#X_train[Feature_Selection(columns, 3)[0]].shape\ntree.fit(X_train[Feature_Selection(columns, 3)[0]], y_train)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=5,\n            splitter='best')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def DecisionTree_accuracy(X):\n    '''Input list of features, output the test accuracy of \n    Decission Tree Classifier and its corresponding features'''\n    #y = df_wine['Class']\n    #Split data into test and train data, 20:80\n    #X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.2)\n    tree.fit(X_train[X], y_train) \n    y_predictfun = tree.predict(X_test[X])\n    test_acc = accuracy_score(y_test, y_predictfun)\n    \n    return test_acc, list(X)\n\n\n",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# selection of Features and their accuracy given number of features chosen\nDecisionTree_accuracy(Feature_Selection(columns,3)[0])",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "(0.92, ['fixed acidity', 'volatile acidity', 'citric acid'])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Question 3/5: What is the AUC of our classifier (using all attributes, also the BEST AUC attainable)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.svm import LinearSVC\n\nsvm = LinearSVC()\nX = df_wine.iloc[:,0:11] #X includes all rows and columns EXCEPT the 'Class' column which we are predicting\ny = df_wine['Class']\n\nX_train, X_test,y_train, y_test = train_test_split(X,y,test_size= 0.2)\n\n#SVM Model\nsvm_model = svm.fit(X_train, y_train)\ny_test_predictions = svm.predict(X_test)\ny_score = svm_model.decision_function(X_test)\nfpr_svm, tpr_svm, thresholds = roc_curve(y_test, y_score)\nroc_auc_svm = auc(fpr_svm, tpr_svm)\n\n#Decision Tree\ntree = DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 2, random_state=5)#HOW TO CHOOSE CRITERION/RANDOM STATE\ntree_model = tree.fit(X_train, y_train)\ny_proba = tree_model.predict_proba(X_test)\ny_score = tree_model.score(X_test, y_test)\n\nfpr_tree, tpr_tree, _ = roc_curve(y_test, y_proba[:,1])\nroc_auc = auc(fpr_tree, tpr_tree)\n\n\nimport matplotlib.pyplot as plt\n% matplotlib inline\nplt.figure()\nplt.plot(fpr_tree, tpr_tree, color='darkorange',\n          lw=2, label='Decision Tree curve zero (area = {0:.2f})'.format(roc_auc))\nplt.plot(fpr_svm, tpr_svm, color='darkred',\n         lw=2, label='SVM ROC curve zero (area = {0:.2f})'.format(roc_auc_svm))\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC for Class Zero and One (Red and White)')\nplt.legend(loc=\"lower right\")\nplt.show()\n",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/matplotlib/font_manager.py:229: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\nUsageError: Line magic function `%` not found.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Question 4: Visualize Decision Tree"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import graphviz \nfrom sklearn import tree as Tree",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#define columns from dataframe - list of all headers; drop target column\ncolumns = list(df_wine)\ncolumns.remove('Class')\ncolumns = np.asarray(columns)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib notebook\n#http://scikit-learn.org/stable/modules/tree.html#tree\ndot_data = Tree.export_graphviz(tree, out_file=None, \n                         feature_names=columns,  \n                         class_names='Class',  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dot_data = Tree.export_graphviz(tree, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"Wine\") ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Question 6: Minimum Number of Attributes Needed to Obtain 95%  Prediction Accuracy"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#RUN SIMULATIONS!!!\nr = 11 #choose up to ll features for the combinations\nX_matrix = []\n\n#num_feat = []\nResults = []\nfor i in range(r):\n    a = 2+i\n    X_matrix.append(Feature_Selection(columns, a ))\nnp.asarray(X_matrix).shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combo_matrix # list of combinations choosing 1-10 features",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combo = {}\nfor j in range(11):\n    combo[j] = combo_matrix[j]\ncombo[9][0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "DecisionTree_accuracy(combo[9][0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Acc = []\nfeat =[]\nfor idx1 in range(11):\n    for idx2 in range(len(combo[a])):\n        acc, features = DecisionTree_accuracy(combo[idx1][idx2])\n        Num_Features = len(features)\n        if acc >= 0.95:\n            Acc.append(acc)\n            feat.append(features)\n    if len(features)> 0:\n        break",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Results_df = pd.DataFrame(\n    {'Accuracy': Acc,\n    'Features':feat})\nResults_df.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "NumFeat = []\nfor i in Results_df['Features']:\n    NumFeat.append(np.asarray(i).shape[0])\nResults_df['Number of Features']= NumFeat\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Results_df.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Results_df[Results_df['Number of Features'] == 2].max()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The minimum number of attributes needed to obtain 95% prediciton accuracy (or greater) is TWO! I wrote a funciton to obtain a list of all feature combination options choosing one or more attribute (Feature_Selection). I then changed X_test/X_train to include only the features in question and fit a new decision tree model and test the accuracy (DecisionTree_accuracy). Adding only the results that had 95% or more accuracy iteratively to empty lists, I created a new data frame (Results_df)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}